{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "afa3a618-0f6b-4d17-b643-48a29f5d19b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Date and Time Manipulation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1d7a6f1a-5f61-4c7d-bb40-da575adaaaaa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* We can use `current_date` to get todayâ€™s server date. \n",
    "  * Date will be returned using **yyyy-MM-dd** format.\n",
    "* We can use `current_timestamp` to get current server time. \n",
    "  * Timestamp will be returned using **yyyy-MM-dd HH:mm:ss:SSS** format.\n",
    "  * Hours will be by default in 24 hour format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5270d927-90a3-4faf-8829-7eafa165df04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "l = [('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "494f0dce-347b-4524-b499-260d4d209e8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l).toDF('dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "766323c7-9754-4d07-be8d-beeb4a2baa62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1db27187-0063-4b55-94b3-3c0d3f8d6784",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------------+\n",
       "|current_date()|\n",
       "+--------------+\n",
       "|    2022-04-20|\n",
       "+--------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+--------------+\n|current_date()|\n+--------------+\n|    2022-04-20|\n+--------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.select(current_date()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a6d9ff0a-160b-4684-8a8a-de7d82583e3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-----------------------+\n",
       "|current_timestamp()    |\n",
       "+-----------------------+\n",
       "|2022-04-20 13:37:45.324|\n",
       "+-----------------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+-----------------------+\n|current_timestamp()    |\n+-----------------------+\n|2022-04-20 13:37:45.324|\n+-----------------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.select(current_timestamp()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "da2dd4ef-fd58-4970-85c4-177aecfab3f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* We can convert a string which contain date or timestamp in non-standard format to standard date or time using `to_date` or `to_timestamp` function respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8f22b38c-2974-4e6d-b212-066c7183e64f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,to_date,to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2af28184-bf2b-4eff-879e-49a8af02721f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+\n",
       "|   to_date|\n",
       "+----------+\n",
       "|2021-02-28|\n",
       "+----------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+\n|   to_date|\n+----------+\n|2021-02-28|\n+----------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.select(to_date(lit('20210228'),'yyyyMMdd').alias('to_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7feb5272-4941-4fbd-9a99-ada3aeaf47db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-------------------+\n",
       "|       to_timestamp|\n",
       "+-------------------+\n",
       "|2021-02-28 17:25:00|\n",
       "+-------------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+-------------------+\n|       to_timestamp|\n+-------------------+\n|2021-02-28 17:25:00|\n+-------------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.select(to_timestamp(lit('20210228 1725'),'yyyyMMdd HHmm').alias('to_timestamp')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "34d69bf3-2a0f-43a7-aa5d-8efaef81ec11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Date and Time Arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d824968d-e97c-41b4-b1f8-d7d223db1de7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* Adding days to a date or timestamp - `date_add`\n",
    "* Subtracting days from a date or timestamp - `date_sub`\n",
    "* Getting difference between 2 dates or timestamps - `datediff`\n",
    "* Getting the number of months between 2 dates or timestamps - `months_between`\n",
    "* Adding months to a date or timestamp - `add_months`\n",
    "* Getting next day from a given date - `next_day`\n",
    "* All the functions are self explanatory. We can apply these on standard date or timestamp. All the functions return date even when applied on timestamp field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f9f2f2ff-ba08-4923-8faf-808670a40a5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2200579b-7320-4811-9558-0a5b273fdaba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datetimesDF = spark.createDataFrame(datetimes, schema=\"date STRING, time STRING\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "44023830-fb11-4bad-bcc9-8993e97c7bf7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+-----------------------+\n",
       "|date      |time                   |\n",
       "+----------+-----------------------+\n",
       "|2014-02-28|2014-02-28 10:00:00.123|\n",
       "|2016-02-29|2016-02-29 08:08:08.999|\n",
       "|2017-10-31|2017-12-31 11:59:59.123|\n",
       "|2019-11-30|2019-08-31 00:00:00.000|\n",
       "+----------+-----------------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+-----------------------+\n|date      |time                   |\n+----------+-----------------------+\n|2014-02-28|2014-02-28 10:00:00.123|\n|2016-02-29|2016-02-29 08:08:08.999|\n|2017-10-31|2017-12-31 11:59:59.123|\n|2019-11-30|2019-08-31 00:00:00.000|\n+----------+-----------------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f3672c05-c68d-4255-a651-6f6c94e0bf57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* Add 10 days to both date and time values.\n",
    "* Subtract 10 days from both date and time values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a8a50918-9043-4ad5-8bbd-04fbf385db3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c6f30f3a-9b9a-4a1b-857d-28bcf95704c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Help on function date_add in module pyspark.sql.functions:\n",
       "\n",
       "date_add(start, days)\n",
       "    Returns the date that is `days` days after `start`\n",
       "    \n",
       "    .. versionadded:: 1.5.0\n",
       "    \n",
       "    Examples\n",
       "    --------\n",
       "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
       "    >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n",
       "    [Row(next_date=datetime.date(2015, 4, 9))]\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Help on function date_add in module pyspark.sql.functions:\n\ndate_add(start, days)\n    Returns the date that is `days` days after `start`\n    \n    .. versionadded:: 1.5.0\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n    >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n    [Row(next_date=datetime.date(2015, 4, 9))]\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "help(date_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "407d0176-667d-4d59-b9af-d0f8681882f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+--------------------+-------------+-------------+-------------+-------------+\n",
       "|      date|                time|date_add_date|time_add_date|date_sub_date|time_sub_date|\n",
       "+----------+--------------------+-------------+-------------+-------------+-------------+\n",
       "|2014-02-28|2014-02-28 10:00:...|   2014-03-10|   2014-03-10|   2014-02-18|   2014-02-18|\n",
       "|2016-02-29|2016-02-29 08:08:...|   2016-03-10|   2016-03-10|   2016-02-19|   2016-02-19|\n",
       "|2017-10-31|2017-12-31 11:59:...|   2017-11-10|   2018-01-10|   2017-10-21|   2017-12-21|\n",
       "|2019-11-30|2019-08-31 00:00:...|   2019-12-10|   2019-09-10|   2019-11-20|   2019-08-21|\n",
       "+----------+--------------------+-------------+-------------+-------------+-------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+--------------------+-------------+-------------+-------------+-------------+\n|      date|                time|date_add_date|time_add_date|date_sub_date|time_sub_date|\n+----------+--------------------+-------------+-------------+-------------+-------------+\n|2014-02-28|2014-02-28 10:00:...|   2014-03-10|   2014-03-10|   2014-02-18|   2014-02-18|\n|2016-02-29|2016-02-29 08:08:...|   2016-03-10|   2016-03-10|   2016-02-19|   2016-02-19|\n|2017-10-31|2017-12-31 11:59:...|   2017-11-10|   2018-01-10|   2017-10-21|   2017-12-21|\n|2019-11-30|2019-08-31 00:00:...|   2019-12-10|   2019-09-10|   2019-11-20|   2019-08-21|\n+----------+--------------------+-------------+-------------+-------------+-------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datetimesDF.\\\n",
    "  withColumn('date_add_date', date_add('date',10)).\\\n",
    "  withColumn('time_add_date',date_add('time',10)).\\\n",
    "  withColumn('date_sub_date',date_sub('date',10)).\\\n",
    "  withColumn('time_sub_date',date_sub('time',10)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7659f931-6dfb-4451-a3b8-709014475b81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* Get the difference between current_date and date values as well as current_timestamp and time values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "debe4627-bb2b-4925-8de2-95e7fa98934e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp, datediff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e40b2c86-437b-4e6d-8fed-581063e62911",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+--------------------+-------------+-------------+\n",
       "|      date|                time|datediff_date|datediff_time|\n",
       "+----------+--------------------+-------------+-------------+\n",
       "|2014-02-28|2014-02-28 10:00:...|         2973|         2973|\n",
       "|2016-02-29|2016-02-29 08:08:...|         2242|         2242|\n",
       "|2017-10-31|2017-12-31 11:59:...|         1632|         1571|\n",
       "|2019-11-30|2019-08-31 00:00:...|          872|          963|\n",
       "+----------+--------------------+-------------+-------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+--------------------+-------------+-------------+\n|      date|                time|datediff_date|datediff_time|\n+----------+--------------------+-------------+-------------+\n|2014-02-28|2014-02-28 10:00:...|         2973|         2973|\n|2016-02-29|2016-02-29 08:08:...|         2242|         2242|\n|2017-10-31|2017-12-31 11:59:...|         1632|         1571|\n|2019-11-30|2019-08-31 00:00:...|          872|          963|\n+----------+--------------------+-------------+-------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datetimesDF.\\\n",
    "  withColumn('datediff_date',datediff(current_date(),'date')).\\\n",
    "  withColumn('datediff_time',datediff(current_timestamp(),'time')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a5574427-2b21-4b94-adb1-f5318a142af9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* Get the number of months between current_date and date values as well as current_timestamp and time values.\n",
    "* Add 3 months to both date values as well as time values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "384040ea-0107-4e1e-b65b-01eb3ebdd95d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import months_between, add_months , round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4b18bd54-4d53-46d4-89b6-ae9aab45c0d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
       "|date      |time                   |months_between_date|months_between_time|add_months_date|add_months_time|\n",
       "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
       "|2014-02-28|2014-02-28 10:00:00.123|97.74              |97.75              |2014-05-28     |2014-05-28     |\n",
       "|2016-02-29|2016-02-29 08:08:08.999|73.71              |73.72              |2016-05-29     |2016-05-29     |\n",
       "|2017-10-31|2017-12-31 11:59:59.123|53.65              |51.65              |2018-01-31     |2018-03-31     |\n",
       "|2019-11-30|2019-08-31 00:00:00.000|28.68              |31.66              |2020-02-29     |2019-11-30     |\n",
       "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n|date      |time                   |months_between_date|months_between_time|add_months_date|add_months_time|\n+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n|2014-02-28|2014-02-28 10:00:00.123|97.74              |97.75              |2014-05-28     |2014-05-28     |\n|2016-02-29|2016-02-29 08:08:08.999|73.71              |73.72              |2016-05-29     |2016-05-29     |\n|2017-10-31|2017-12-31 11:59:59.123|53.65              |51.65              |2018-01-31     |2018-03-31     |\n|2019-11-30|2019-08-31 00:00:00.000|28.68              |31.66              |2020-02-29     |2019-11-30     |\n+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datetimesDF.\\\n",
    "  withColumn('months_between_date',round(months_between(current_date(),'date'),2)).\\\n",
    " withColumn('months_between_time',round(months_between(current_timestamp(),'time'),2)).\\\n",
    " withColumn('add_months_date',add_months('date',3)).\\\n",
    " withColumn('add_months_time',add_months('time',3)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c4f00b3f-5734-4d44-a26a-7a4849e9a069",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Using Date and Time Trunc Functions\n",
    "In Data Warehousing we quite often run to date reports such as week to date, month to date, year to date etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "48996402-87c1-40c8-8024-50e0df0fe1aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* We can use `trunc` or `date_trunc` for the same to get the beginning date of the week, month, current year etc by passing date or timestamp to it.\n",
    "* We can use `trunc` to get beginning date of the month or year by passing date or timestamp to it - for example `trunc(current_date(), \"MM\")` will give the first of the current month.\n",
    "* We can use `date_trunc` to get beginning date of the month or year as well as beginning time of the day or hour by passing timestamp to it.\n",
    "  * Get beginning date based on month - `date_trunc(\"MM\", current_timestamp())`\n",
    "  * Get beginning time based on day - `date_trunc(\"DAY\", current_timestamp())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0335cb49-0d68-427e-9bee-e94dee592020",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trunc, date_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a658ad0c-08cd-4645-a193-b1994d2f0ae1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Help on function trunc in module pyspark.sql.functions:\n",
       "\n",
       "trunc(date, format)\n",
       "    Returns date truncated to the unit specified by the format.\n",
       "    \n",
       "    .. versionadded:: 1.5.0\n",
       "    \n",
       "    Parameters\n",
       "    ----------\n",
       "    date : :class:`~pyspark.sql.Column` or str\n",
       "    format : str\n",
       "        'year', 'yyyy', 'yy' to truncate by year,\n",
       "        or 'month', 'mon', 'mm' to truncate by month\n",
       "        Other options are: 'week', 'quarter'\n",
       "    \n",
       "    Examples\n",
       "    --------\n",
       "    >>> df = spark.createDataFrame([('1997-02-28',)], ['d'])\n",
       "    >>> df.select(trunc(df.d, 'year').alias('year')).collect()\n",
       "    [Row(year=datetime.date(1997, 1, 1))]\n",
       "    >>> df.select(trunc(df.d, 'mon').alias('month')).collect()\n",
       "    [Row(month=datetime.date(1997, 2, 1))]\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Help on function trunc in module pyspark.sql.functions:\n\ntrunc(date, format)\n    Returns date truncated to the unit specified by the format.\n    \n    .. versionadded:: 1.5.0\n    \n    Parameters\n    ----------\n    date : :class:`~pyspark.sql.Column` or str\n    format : str\n        'year', 'yyyy', 'yy' to truncate by year,\n        or 'month', 'mon', 'mm' to truncate by month\n        Other options are: 'week', 'quarter'\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('1997-02-28',)], ['d'])\n    >>> df.select(trunc(df.d, 'year').alias('year')).collect()\n    [Row(year=datetime.date(1997, 1, 1))]\n    >>> df.select(trunc(df.d, 'mon').alias('month')).collect()\n    [Row(month=datetime.date(1997, 2, 1))]\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "help(trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f3372577-5e1c-43ae-bf47-94fe0f48aa63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Help on function date_trunc in module pyspark.sql.functions:\n",
       "\n",
       "date_trunc(format, timestamp)\n",
       "    Returns timestamp truncated to the unit specified by the format.\n",
       "    \n",
       "    .. versionadded:: 2.3.0\n",
       "    \n",
       "    Parameters\n",
       "    ----------\n",
       "    format : str\n",
       "        'year', 'yyyy', 'yy' to truncate by year,\n",
       "        'month', 'mon', 'mm' to truncate by month,\n",
       "        'day', 'dd' to truncate by day,\n",
       "        Other options are:\n",
       "        'microsecond', 'millisecond', 'second', 'minute', 'hour', 'week', 'quarter'\n",
       "    timestamp : :class:`~pyspark.sql.Column` or str\n",
       "    \n",
       "    Examples\n",
       "    --------\n",
       "    >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n",
       "    >>> df.select(date_trunc('year', df.t).alias('year')).collect()\n",
       "    [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\n",
       "    >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\n",
       "    [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Help on function date_trunc in module pyspark.sql.functions:\n\ndate_trunc(format, timestamp)\n    Returns timestamp truncated to the unit specified by the format.\n    \n    .. versionadded:: 2.3.0\n    \n    Parameters\n    ----------\n    format : str\n        'year', 'yyyy', 'yy' to truncate by year,\n        'month', 'mon', 'mm' to truncate by month,\n        'day', 'dd' to truncate by day,\n        Other options are:\n        'microsecond', 'millisecond', 'second', 'minute', 'hour', 'week', 'quarter'\n    timestamp : :class:`~pyspark.sql.Column` or str\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n    >>> df.select(date_trunc('year', df.t).alias('year')).collect()\n    [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\n    >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\n    [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "help(date_trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "35759dea-7afb-40d1-af93-c9d88ba0b767",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+-----------------------+\n",
       "|date      |time                   |\n",
       "+----------+-----------------------+\n",
       "|2014-02-28|2014-02-28 10:00:00.123|\n",
       "|2016-02-29|2016-02-29 08:08:08.999|\n",
       "|2017-10-31|2017-12-31 11:59:59.123|\n",
       "|2019-11-30|2019-08-31 00:00:00.000|\n",
       "+----------+-----------------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+-----------------------+\n|date      |time                   |\n+----------+-----------------------+\n|2014-02-28|2014-02-28 10:00:00.123|\n|2016-02-29|2016-02-29 08:08:08.999|\n|2017-10-31|2017-12-31 11:59:59.123|\n|2019-11-30|2019-08-31 00:00:00.000|\n+----------+-----------------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "43b92285-b854-4ee1-b74f-8740c8eca897",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c19ce471-46a0-4297-9508-4aa228038a0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+-----------------------+----------+----------+\n",
       "|date      |time                   |date_trunc|time_trunc|\n",
       "+----------+-----------------------+----------+----------+\n",
       "|2014-02-28|2014-02-28 10:00:00.123|2014-02-01|2014-01-01|\n",
       "|2016-02-29|2016-02-29 08:08:08.999|2016-02-01|2016-01-01|\n",
       "|2017-10-31|2017-12-31 11:59:59.123|2017-10-01|2017-01-01|\n",
       "|2019-11-30|2019-08-31 00:00:00.000|2019-11-01|2019-01-01|\n",
       "+----------+-----------------------+----------+----------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+-----------------------+----------+----------+\n|date      |time                   |date_trunc|time_trunc|\n+----------+-----------------------+----------+----------+\n|2014-02-28|2014-02-28 10:00:00.123|2014-02-01|2014-01-01|\n|2016-02-29|2016-02-29 08:08:08.999|2016-02-01|2016-01-01|\n|2017-10-31|2017-12-31 11:59:59.123|2017-10-01|2017-01-01|\n|2019-11-30|2019-08-31 00:00:00.000|2019-11-01|2019-01-01|\n+----------+-----------------------+----------+----------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datetimesDF.\\\n",
    "  withColumn('date_trunc',trunc('date','MM')).\\\n",
    "  withColumn('time_trunc',trunc('time','yy')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c240dcbd-9511-4777-ad0d-f76f0292e437",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Get begginning hour time using date and time field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "424f9dc3-ea5e-424d-b982-3e01cf503b45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e50c2eac-4378-4740-9865-c421538baa1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+-----------------------+-------------------+-------------------+\n",
       "|date      |time                   |date_trunc         |time_trunc         |\n",
       "+----------+-----------------------+-------------------+-------------------+\n",
       "|2014-02-28|2014-02-28 10:00:00.123|2014-02-01 00:00:00|2014-01-01 00:00:00|\n",
       "|2016-02-29|2016-02-29 08:08:08.999|2016-02-01 00:00:00|2016-01-01 00:00:00|\n",
       "|2017-10-31|2017-12-31 11:59:59.123|2017-10-01 00:00:00|2017-01-01 00:00:00|\n",
       "|2019-11-30|2019-08-31 00:00:00.000|2019-11-01 00:00:00|2019-01-01 00:00:00|\n",
       "+----------+-----------------------+-------------------+-------------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+-----------------------+-------------------+-------------------+\n|date      |time                   |date_trunc         |time_trunc         |\n+----------+-----------------------+-------------------+-------------------+\n|2014-02-28|2014-02-28 10:00:00.123|2014-02-01 00:00:00|2014-01-01 00:00:00|\n|2016-02-29|2016-02-29 08:08:08.999|2016-02-01 00:00:00|2016-01-01 00:00:00|\n|2017-10-31|2017-12-31 11:59:59.123|2017-10-01 00:00:00|2017-01-01 00:00:00|\n|2019-11-30|2019-08-31 00:00:00.000|2019-11-01 00:00:00|2019-01-01 00:00:00|\n+----------+-----------------------+-------------------+-------------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datetimesDF.\\\n",
    " withColumn('date_trunc',date_trunc('MM','date')).\\\n",
    " withColumn('time_trunc',date_trunc('yy','time')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f489c5e5-5bf1-4fdb-ae17-fde851fe509a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+--------------------+-------------------+-------------------+-------------------+\n",
       "|      date|                time|            date_dt|            time_dt|           time_dt1|\n",
       "+----------+--------------------+-------------------+-------------------+-------------------+\n",
       "|2014-02-28|2014-02-28 10:00:...|2014-02-28 00:00:00|2014-02-28 10:00:00|2014-02-28 00:00:00|\n",
       "|2016-02-29|2016-02-29 08:08:...|2016-02-29 00:00:00|2016-02-29 08:00:00|2016-02-29 00:00:00|\n",
       "|2017-10-31|2017-12-31 11:59:...|2017-10-31 00:00:00|2017-12-31 11:00:00|2017-12-31 00:00:00|\n",
       "|2019-11-30|2019-08-31 00:00:...|2019-11-30 00:00:00|2019-08-31 00:00:00|2019-08-31 00:00:00|\n",
       "+----------+--------------------+-------------------+-------------------+-------------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+--------------------+-------------------+-------------------+-------------------+\n|      date|                time|            date_dt|            time_dt|           time_dt1|\n+----------+--------------------+-------------------+-------------------+-------------------+\n|2014-02-28|2014-02-28 10:00:...|2014-02-28 00:00:00|2014-02-28 10:00:00|2014-02-28 00:00:00|\n|2016-02-29|2016-02-29 08:08:...|2016-02-29 00:00:00|2016-02-29 08:00:00|2016-02-29 00:00:00|\n|2017-10-31|2017-12-31 11:59:...|2017-10-31 00:00:00|2017-12-31 11:00:00|2017-12-31 00:00:00|\n|2019-11-30|2019-08-31 00:00:...|2019-11-30 00:00:00|2019-08-31 00:00:00|2019-08-31 00:00:00|\n+----------+--------------------+-------------------+-------------------+-------------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datetimesDF.\\\n",
    "  withColumn('date_dt',date_trunc('HOUR','date')).\\\n",
    "  withColumn('time_dt',date_trunc('HOUR','time')).\\\n",
    "  withColumn('time_dt1', date_trunc('dd','time')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "01d7efbb-2806-4e6e-bcf3-61eb8c22d265",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* `year`\n",
    "* `month`\n",
    "* `weekofyear`\n",
    "* `dayofyear`\n",
    "* `dayofmonth`\n",
    "* `dayofweek`\n",
    "* `hour`\n",
    "* `minute`\n",
    "* `second`\n",
    "\n",
    "There might be few more functions. You can review based up on your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b0f49484-f49a-4dee-9ab0-a929fe9d28b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([('X',)],['dummy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5d392add-93b2-4816-b829-1db6878c4e04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "|    X|\n",
       "+-----+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+-----+\n|dummy|\n+-----+\n|    X|\n+-----+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7335f568-dca3-458f-b002-c363dd508902",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year,month,weekofyear,dayofmonth,dayofyear,dayofweek,current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3f2ff42c-17aa-47e9-ba2c-d392b90ed32a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------------+----+-----+----------+----------+---------+---------+\n",
       "|current_date()|year|month|weekofyear|dayofmonth|dayofyear|dayofweek|\n",
       "+--------------+----+-----+----------+----------+---------+---------+\n",
       "|    2022-04-20|2022|    4|        16|        20|      110|        4|\n",
       "+--------------+----+-----+----------+----------+---------+---------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+--------------+----+-----+----------+----------+---------+---------+\n|current_date()|year|month|weekofyear|dayofmonth|dayofyear|dayofweek|\n+--------------+----+-----+----------+----------+---------+---------+\n|    2022-04-20|2022|    4|        16|        20|      110|        4|\n+--------------+----+-----+----------+----------+---------+---------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.select(\n",
    " current_date(),\\\n",
    "  year(current_date()).alias('year'),\\\n",
    "  month(current_date()).alias('month'),\\\n",
    "  weekofyear(current_date()).alias('weekofyear'),\\\n",
    "  dayofmonth(current_date()).alias('dayofmonth'),\\\n",
    "  dayofyear(current_date()).alias('dayofyear'),\\\n",
    "  dayofweek(current_date()).alias('dayofweek')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "42f50ebc-b8a1-49c5-8951-e137ccde5edf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Help on function weekofyear in module pyspark.sql.functions:\n",
       "\n",
       "weekofyear(col)\n",
       "    Extract the week number of a given date as integer.\n",
       "    \n",
       "    .. versionadded:: 1.5.0\n",
       "    \n",
       "    Examples\n",
       "    --------\n",
       "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
       "    >>> df.select(weekofyear(df.dt).alias('week')).collect()\n",
       "    [Row(week=15)]\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Help on function weekofyear in module pyspark.sql.functions:\n\nweekofyear(col)\n    Extract the week number of a given date as integer.\n    \n    .. versionadded:: 1.5.0\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n    >>> df.select(weekofyear(df.dt).alias('week')).collect()\n    [Row(week=15)]\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "help(weekofyear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0fee134c-1fac-4864-918f-288fa84bf342",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp,hour,minute,second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "53fb6ec2-28d4-4d75-8f26-f8874e9117ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-----------------------+----+-----+----------+----+------+------+\n",
       "|current_timestamp      |year|month|dayofmonth|hour|minute|second|\n",
       "+-----------------------+----+-----+----------+----+------+------+\n",
       "|2022-04-20 13:38:05.065|2022|4    |20        |13  |38    |5     |\n",
       "+-----------------------+----+-----+----------+----+------+------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+-----------------------+----+-----+----------+----+------+------+\n|current_timestamp      |year|month|dayofmonth|hour|minute|second|\n+-----------------------+----+-----+----------+----+------+------+\n|2022-04-20 13:38:05.065|2022|4    |20        |13  |38    |5     |\n+-----------------------+----+-----+----------+----+------+------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.select(\n",
    "    current_timestamp().alias('current_timestamp'), \n",
    "    year(current_timestamp()).alias('year'),\n",
    "    month(current_timestamp()).alias('month'),\n",
    "    dayofmonth(current_timestamp()).alias('dayofmonth'),\n",
    "    hour(current_timestamp()).alias('hour'),\n",
    "    minute(current_timestamp()).alias('minute'),\n",
    "    second(current_timestamp()).alias('second')\n",
    ").show(truncate=False) #yyyy-MM-dd HH:mm:ss.SSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "73b0e539-6c91-4318-be28-192f4b708ceb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Using to_date and to_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c70a436c-0c8f-459e-8106-d89b79debfc8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* `yyyy-MM-dd` is the standard date format\n",
    "* `yyyy-MM-dd HH:mm:ss.SSS` is the standard timestamp format\n",
    "* Most of the date manipulation functions expect date and time using standard format. However, we might not have data in the expected standard format.\n",
    "* In those scenarios we can use `to_date` and `to_timestamp` to convert non standard dates and timestamps to standard ones respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1b2ec328-4fbf-412d-a3dc-eb4a01113d45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datetimes = [(20140228, \"28-Feb-2014 10:00:00.123\"),\n",
    "                     (20160229, \"20-Feb-2016 08:08:08.999\"),\n",
    "                     (20171031, \"31-Dec-2017 11:59:59.123\"),\n",
    "                     (20191130, \"31-Aug-2019 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3908b134-0a14-4d76-957a-ab3477159711",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datetimesDF = spark.createDataFrame(datetimes, schema=\"date BIGINT, time STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4806dbb8-ac51-4308-abfb-767358c7b7bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------+------------------------+\n",
       "|date    |time                    |\n",
       "+--------+------------------------+\n",
       "|20140228|28-Feb-2014 10:00:00.123|\n",
       "|20160229|20-Feb-2016 08:08:08.999|\n",
       "|20171031|31-Dec-2017 11:59:59.123|\n",
       "|20191130|31-Aug-2019 00:00:00.000|\n",
       "+--------+------------------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+--------+------------------------+\n|date    |time                    |\n+--------+------------------------+\n|20140228|28-Feb-2014 10:00:00.123|\n|20160229|20-Feb-2016 08:08:08.999|\n|20171031|31-Dec-2017 11:59:59.123|\n|20191130|31-Aug-2019 00:00:00.000|\n+--------+------------------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "42b26a8a-b72c-4c41-a78e-6c99c39e1767",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0aded26d-816a-4529-bd69-05eb57d48a2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "|    X|\n",
       "+-----+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+-----+\n|dummy|\n+-----+\n|    X|\n+-----+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fa46f1df-2056-43f1-9325-9438be7c84f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+\n",
       "|   to_date|\n",
       "+----------+\n",
       "|2021-08-09|\n",
       "+----------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+\n|   to_date|\n+----------+\n|2021-08-09|\n+----------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.select(to_date(lit('20210809'),'yyyyMMdd').alias('to_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8b7edc0f-f15d-424e-ba06-90b8f7238d10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+\n",
       "|   to_date|\n",
       "+----------+\n",
       "|2021-03-02|\n",
       "+----------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+\n|   to_date|\n+----------+\n|2021-03-02|\n+----------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# year and day of year to standard date\n",
    "df.select(to_date(lit('2021061'),'yyyyDDD').alias('to_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0e478813-e4de-4d3a-ba5a-a7f7a48c6223",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+\n",
       "|   to_date|\n",
       "+----------+\n",
       "|2022-03-02|\n",
       "+----------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+\n|   to_date|\n+----------+\n|2022-03-02|\n+----------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.select(to_date(lit('02/03/2022'),'dd/MM/yyyy').alias('to_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a7178a5e-c6c1-4e5f-b803-c39d0b391017",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+\n",
       "|   to_date|\n",
       "+----------+\n",
       "|2021-03-02|\n",
       "+----------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+\n|   to_date|\n+----------+\n|2021-03-02|\n+----------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.select(to_date(lit('02-03-2021'),'dd-MM-yyyy').alias('to_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cce0521d-34fa-46a1-b223-f305edefdda3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+\n",
       "|   to_date|\n",
       "+----------+\n",
       "|2021-03-02|\n",
       "+----------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+\n|   to_date|\n+----------+\n|2021-03-02|\n+----------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.select(to_date(lit('02-Mar-2021'),'dd-MMM-yyyy').alias('to_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "58751373-7b6c-48ef-8532-0c97c29b51d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+\n",
       "|   to_date|\n",
       "+----------+\n",
       "|2021-03-02|\n",
       "+----------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+\n|   to_date|\n+----------+\n|2021-03-02|\n+----------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.select(to_date(lit('02-March-2021'),'dd-MMMM-yyyy').alias('to_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6d987464-d46f-4abe-b443-d5a985003511",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+\n",
       "|   to_date|\n",
       "+----------+\n",
       "|2022-03-02|\n",
       "+----------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+\n|   to_date|\n+----------+\n|2022-03-02|\n+----------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.select(to_date(lit('March 2,2022'),'MMMM d,yyyy').alias('to_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c88ff1b0-3b2c-4b50-be00-c0c6156f9594",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3a7fdd64-bd5c-4947-b5b8-cc899e8d937e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-------------------+\n",
       "|            to_date|\n",
       "+-------------------+\n",
       "|2022-04-02 00:00:00|\n",
       "+-------------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+-------------------+\n|            to_date|\n+-------------------+\n|2022-04-02 00:00:00|\n+-------------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.select(to_timestamp(lit('02-Apr-2022'),'dd-MMM-yyyy').alias('to_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9982b778-f3bf-4cbf-8229-14d97523afa1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-------------------+\n",
       "|            to_date|\n",
       "+-------------------+\n",
       "|2021-03-02 17:30:15|\n",
       "+-------------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+-------------------+\n|            to_date|\n+-------------------+\n|2021-03-02 17:30:15|\n+-------------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.select(to_timestamp(lit('02-Mar-2021 17:30:15'),'dd-MMM-yyyy HH:mm:ss').alias('to_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fc8518d4-838f-48f0-856e-72da1beb4641",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "63cc511d-9e89-45f0-9e1c-8f879d486ac9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------+------------------------+----------+-----------------------+\n",
       "|date    |time                    |to_date   |to_timestamp           |\n",
       "+--------+------------------------+----------+-----------------------+\n",
       "|20140228|28-Feb-2014 10:00:00.123|2014-02-28|2014-02-28 10:00:00.123|\n",
       "|20160229|20-Feb-2016 08:08:08.999|2016-02-29|2016-02-20 08:08:08.999|\n",
       "|20171031|31-Dec-2017 11:59:59.123|2017-10-31|2017-12-31 11:59:59.123|\n",
       "|20191130|31-Aug-2019 00:00:00.000|2019-11-30|2019-08-31 00:00:00    |\n",
       "+--------+------------------------+----------+-----------------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+--------+------------------------+----------+-----------------------+\n|date    |time                    |to_date   |to_timestamp           |\n+--------+------------------------+----------+-----------------------+\n|20140228|28-Feb-2014 10:00:00.123|2014-02-28|2014-02-28 10:00:00.123|\n|20160229|20-Feb-2016 08:08:08.999|2016-02-29|2016-02-20 08:08:08.999|\n|20171031|31-Dec-2017 11:59:59.123|2017-10-31|2017-12-31 11:59:59.123|\n|20191130|31-Aug-2019 00:00:00.000|2019-11-30|2019-08-31 00:00:00    |\n+--------+------------------------+----------+-----------------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datetimesDF.\\\n",
    "  withColumn('to_date',to_date(col('date').cast('string'),'yyyyMMdd')).\\\n",
    "  withColumn('to_timestamp',to_timestamp(col('time'),'dd-MMM-yyyy HH:mm:ss.SSS')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b108c481-5514-47bb-abe4-7a654c66d8c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* We can use `date_format` to extract the required information in a desired format from standard date or timestamp. Earlier we have explored `to_date` and `to_timestamp` to convert non standard date or timestamp to standard ones respectively.\n",
    "* There are also specific functions to extract year, month, day with in a week, a day with in a month, day with in a year etc. These are covered as part of earlier topics in this section or module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "24cbcc70-a41c-4c11-ab4e-a94258bf9363",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e546ee1b-7e1b-42db-bdb1-58dafabb0f47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datetimesDF = spark.createDataFrame(datetimes, schema=\"date STRING, time STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b49b3661-ff89-4136-ae74-81b1f0865ed3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+-----------------------+\n",
       "|date      |time                   |\n",
       "+----------+-----------------------+\n",
       "|2014-02-28|2014-02-28 10:00:00.123|\n",
       "|2016-02-29|2016-02-29 08:08:08.999|\n",
       "|2017-10-31|2017-12-31 11:59:59.123|\n",
       "|2019-11-30|2019-08-31 00:00:00.000|\n",
       "+----------+-----------------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+-----------------------+\n|date      |time                   |\n+----------+-----------------------+\n|2014-02-28|2014-02-28 10:00:00.123|\n|2016-02-29|2016-02-29 08:08:08.999|\n|2017-10-31|2017-12-31 11:59:59.123|\n|2019-11-30|2019-08-31 00:00:00.000|\n+----------+-----------------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aa01e8a5-dd41-4941-8108-021e71e48a6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "adf49a82-9f9f-4079-8277-33ff69a66b1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+-----------------------+-------+-------+\n",
       "|date      |time                   |date_ym|time_ym|\n",
       "+----------+-----------------------+-------+-------+\n",
       "|2014-02-28|2014-02-28 10:00:00.123|201402 |201402 |\n",
       "|2016-02-29|2016-02-29 08:08:08.999|201602 |201602 |\n",
       "|2017-10-31|2017-12-31 11:59:59.123|201710 |201712 |\n",
       "|2019-11-30|2019-08-31 00:00:00.000|201911 |201908 |\n",
       "+----------+-----------------------+-------+-------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+-----------------------+-------+-------+\n|date      |time                   |date_ym|time_ym|\n+----------+-----------------------+-------+-------+\n|2014-02-28|2014-02-28 10:00:00.123|201402 |201402 |\n|2016-02-29|2016-02-29 08:08:08.999|201602 |201602 |\n|2017-10-31|2017-12-31 11:59:59.123|201710 |201712 |\n|2019-11-30|2019-08-31 00:00:00.000|201911 |201908 |\n+----------+-----------------------+-------+-------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datetimesDF.\\\n",
    "  withColumn('date_ym',date_format('date','yyyyMM')).\\\n",
    "  withColumn('time_ym',date_format('time','yyyyMM')).\\\n",
    "show(truncate=False)\n",
    "\n",
    "# yyyy\n",
    "# MM\n",
    "# dd\n",
    "# DD\n",
    "# HH\n",
    "# hh\n",
    "# mm\n",
    "# ss\n",
    "# SSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "da718697-6b94-42af-98e0-53ae9bdc933c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+-----------------------+--------------+--------------+\n",
       "|date      |time                   |date_dt       |date_ts       |\n",
       "+----------+-----------------------+--------------+--------------+\n",
       "|2014-02-28|2014-02-28 10:00:00.123|20140228000000|20140228100000|\n",
       "|2016-02-29|2016-02-29 08:08:08.999|20160229000000|20160229080808|\n",
       "|2017-10-31|2017-12-31 11:59:59.123|20171031000000|20171231115959|\n",
       "|2019-11-30|2019-08-31 00:00:00.000|20191130000000|20190831000000|\n",
       "+----------+-----------------------+--------------+--------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+-----------------------+--------------+--------------+\n|date      |time                   |date_dt       |date_ts       |\n+----------+-----------------------+--------------+--------------+\n|2014-02-28|2014-02-28 10:00:00.123|20140228000000|20140228100000|\n|2016-02-29|2016-02-29 08:08:08.999|20160229000000|20160229080808|\n|2017-10-31|2017-12-31 11:59:59.123|20171031000000|20171231115959|\n|2019-11-30|2019-08-31 00:00:00.000|20191130000000|20190831000000|\n+----------+-----------------------+--------------+--------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datetimesDF.\\\n",
    "  withColumn('date_dt',date_format('date','yyyyMMddHHmmss')).\\\n",
    "  withColumn('date_ts',date_format('time','yyyyMMddHHmmss')).\\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "57ae4f35-999d-4bbb-9653-93ce2423e8eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+-----------------------+--------------+--------------+\n",
       "|date      |time                   |date_dt       |date_ts       |\n",
       "+----------+-----------------------+--------------+--------------+\n",
       "|2014-02-28|2014-02-28 10:00:00.123|20140228000000|20140228100000|\n",
       "|2016-02-29|2016-02-29 08:08:08.999|20160229000000|20160229080808|\n",
       "|2017-10-31|2017-12-31 11:59:59.123|20171031000000|20171231115959|\n",
       "|2019-11-30|2019-08-31 00:00:00.000|20191130000000|20190831000000|\n",
       "+----------+-----------------------+--------------+--------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+-----------------------+--------------+--------------+\n|date      |time                   |date_dt       |date_ts       |\n+----------+-----------------------+--------------+--------------+\n|2014-02-28|2014-02-28 10:00:00.123|20140228000000|20140228100000|\n|2016-02-29|2016-02-29 08:08:08.999|20160229000000|20160229080808|\n|2017-10-31|2017-12-31 11:59:59.123|20171031000000|20171231115959|\n|2019-11-30|2019-08-31 00:00:00.000|20191130000000|20190831000000|\n+----------+-----------------------+--------------+--------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"date_dt\", date_format(\"date\", \"yyyyMMddHHmmss\").cast('long')). \\\n",
    "    withColumn(\"date_ts\", date_format(\"time\", \"yyyyMMddHHmmss\").cast('long')). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "48c33034-8ac7-4a54-b0f2-91d6a7889a0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fbf93e4d-ab39-48e5-9bb5-35a77aba84a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+-----------------------+-------+-------+\n",
       "|date      |time                   |date_yd|time_yd|\n",
       "+----------+-----------------------+-------+-------+\n",
       "|2014-02-28|2014-02-28 10:00:00.123|2014059|2014059|\n",
       "|2016-02-29|2016-02-29 08:08:08.999|2016060|2016060|\n",
       "|2017-10-31|2017-12-31 11:59:59.123|2017304|2017365|\n",
       "|2019-11-30|2019-08-31 00:00:00.000|2019334|2019243|\n",
       "+----------+-----------------------+-------+-------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+-----------------------+-------+-------+\n|date      |time                   |date_yd|time_yd|\n+----------+-----------------------+-------+-------+\n|2014-02-28|2014-02-28 10:00:00.123|2014059|2014059|\n|2016-02-29|2016-02-29 08:08:08.999|2016060|2016060|\n|2017-10-31|2017-12-31 11:59:59.123|2017304|2017365|\n|2019-11-30|2019-08-31 00:00:00.000|2019334|2019243|\n+----------+-----------------------+-------+-------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datetimesDF.\\\n",
    "  withColumn('date_yd',date_format('date','yyyyDDD').cast('int')).\\\n",
    "  withColumn('time_yd',date_format('time','yyyyDDD').cast('int')).\\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "89a11d9d-3c46-4e28-a23a-d7e4844de6b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "get complete description of the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "76aef346-1484-4784-91b4-34070738fb69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+-----------------------+-----------------+\n",
       "|date      |time                   |date_desc        |\n",
       "+----------+-----------------------+-----------------+\n",
       "|2014-02-28|2014-02-28 10:00:00.123|February 28, 2014|\n",
       "|2016-02-29|2016-02-29 08:08:08.999|February 29, 2016|\n",
       "|2017-10-31|2017-12-31 11:59:59.123|October 31, 2017 |\n",
       "|2019-11-30|2019-08-31 00:00:00.000|November 30, 2019|\n",
       "+----------+-----------------------+-----------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+-----------------------+-----------------+\n|date      |time                   |date_desc        |\n+----------+-----------------------+-----------------+\n|2014-02-28|2014-02-28 10:00:00.123|February 28, 2014|\n|2016-02-29|2016-02-29 08:08:08.999|February 29, 2016|\n|2017-10-31|2017-12-31 11:59:59.123|October 31, 2017 |\n|2019-11-30|2019-08-31 00:00:00.000|November 30, 2019|\n+----------+-----------------------+-----------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datetimesDF.\\\n",
    "  withColumn('date_desc',date_format('date','MMMM dd, yyyy')).\\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2a47c624-f70a-4310-b6f4-a54a11bddf63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+--------------------+-------------+\n",
       "|      date|                time|day_name_abbr|\n",
       "+----------+--------------------+-------------+\n",
       "|2014-02-28|2014-02-28 10:00:...|          Fri|\n",
       "|2016-02-29|2016-02-29 08:08:...|          Mon|\n",
       "|2017-10-31|2017-12-31 11:59:...|          Tue|\n",
       "|2019-11-30|2019-08-31 00:00:...|          Sat|\n",
       "+----------+--------------------+-------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+--------------------+-------------+\n|      date|                time|day_name_abbr|\n+----------+--------------------+-------------+\n|2014-02-28|2014-02-28 10:00:...|          Fri|\n|2016-02-29|2016-02-29 08:08:...|          Mon|\n|2017-10-31|2017-12-31 11:59:...|          Tue|\n|2019-11-30|2019-08-31 00:00:...|          Sat|\n+----------+--------------------+-------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# name of the week day\n",
    "\n",
    "datetimesDF.\\\n",
    "  withColumn('day_name_abbr',date_format('date','EE')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7492cf11-2b84-403b-a656-9ec2457ef3ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+--------------------+-------------+\n",
       "|      date|                time|day_full_name|\n",
       "+----------+--------------------+-------------+\n",
       "|2014-02-28|2014-02-28 10:00:...|       Friday|\n",
       "|2016-02-29|2016-02-29 08:08:...|       Monday|\n",
       "|2017-10-31|2017-12-31 11:59:...|      Tuesday|\n",
       "|2019-11-30|2019-08-31 00:00:...|     Saturday|\n",
       "+----------+--------------------+-------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+--------------------+-------------+\n|      date|                time|day_full_name|\n+----------+--------------------+-------------+\n|2014-02-28|2014-02-28 10:00:...|       Friday|\n|2016-02-29|2016-02-29 08:08:...|       Monday|\n|2017-10-31|2017-12-31 11:59:...|      Tuesday|\n|2019-11-30|2019-08-31 00:00:...|     Saturday|\n+----------+--------------------+-------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datetimesDF.\\\n",
    "  withColumn('day_full_name',date_format('date','EEEE')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "457b10f9-cbdc-441a-834d-45861b90b7db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Dealing with Unix Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "298a6967-a354-4209-93b1-03d965ef51b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* It is an integer and started from January 1st 1970 Midnight UTC.\n",
    "* Beginning time is also known as epoch and is incremented by 1 every second.\n",
    "* We can convert Unix Timestamp to regular date or timestamp and vice versa.\n",
    "* We can use `unix_timestamp` to convert regular date or timestamp to a unix timestamp value. For example `unix_timestamp(lit(\"2019-11-19 00:00:00\"))`\n",
    "* We can use `from_unixtime` to convert unix timestamp to regular date or timestamp. For example `from_unixtime(lit(1574101800))`\n",
    "* We can also pass format to both the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f3d252a1-b775-4d65-9d6e-7b8c1a7d865e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datetimes = [(20140228, \"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (20160229, \"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (20171031, \"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (20191130, \"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "355ba1a8-706f-4e8e-86ae-46c6aa215a3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datetimesDF = spark.createDataFrame(datetimes).toDF(\"dateid\", \"date\", \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9cad84f8-3ff5-4e6f-8235-a34468bf0342",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------+----------+-----------------------+\n",
       "|dateid  |date      |time                   |\n",
       "+--------+----------+-----------------------+\n",
       "|20140228|2014-02-28|2014-02-28 10:00:00.123|\n",
       "|20160229|2016-02-29|2016-02-29 08:08:08.999|\n",
       "|20171031|2017-10-31|2017-12-31 11:59:59.123|\n",
       "|20191130|2019-11-30|2019-08-31 00:00:00.000|\n",
       "+--------+----------+-----------------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+--------+----------+-----------------------+\n|dateid  |date      |time                   |\n+--------+----------+-----------------------+\n|20140228|2014-02-28|2014-02-28 10:00:00.123|\n|20160229|2016-02-29|2016-02-29 08:08:08.999|\n|20171031|2017-10-31|2017-12-31 11:59:59.123|\n|20191130|2019-11-30|2019-08-31 00:00:00.000|\n+--------+----------+-----------------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2e0c4bb5-57eb-4df5-8812-c3af631f54c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ce2c1275-ab1b-433b-92f6-8bcb830cd693",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------+----------+--------------------+------------+----------+----------+\n",
       "|  dateid|      date|                time|unix_date_id| unix_date| unix_time|\n",
       "+--------+----------+--------------------+------------+----------+----------+\n",
       "|20140228|2014-02-28|2014-02-28 10:00:...|  1393545600|1393545600|1393581600|\n",
       "|20160229|2016-02-29|2016-02-29 08:08:...|  1456704000|1456704000|1456733288|\n",
       "|20171031|2017-10-31|2017-12-31 11:59:...|  1509408000|1509408000|1514721599|\n",
       "|20191130|2019-11-30|2019-08-31 00:00:...|  1575072000|1575072000|1567209600|\n",
       "+--------+----------+--------------------+------------+----------+----------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+--------+----------+--------------------+------------+----------+----------+\n|  dateid|      date|                time|unix_date_id| unix_date| unix_time|\n+--------+----------+--------------------+------------+----------+----------+\n|20140228|2014-02-28|2014-02-28 10:00:...|  1393545600|1393545600|1393581600|\n|20160229|2016-02-29|2016-02-29 08:08:...|  1456704000|1456704000|1456733288|\n|20171031|2017-10-31|2017-12-31 11:59:...|  1509408000|1509408000|1514721599|\n|20191130|2019-11-30|2019-08-31 00:00:...|  1575072000|1575072000|1567209600|\n+--------+----------+--------------------+------------+----------+----------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datetimesDF.\\\n",
    "  withColumn('unix_date_id',unix_timestamp(col('dateid').cast('string'),'yyyyMMdd')).\\\n",
    "  withColumn('unix_date',unix_timestamp('date','yyyy-MM-dd')).\\\n",
    "  withColumn('unix_time',unix_timestamp('time','yyyy-MM-dd HH:mm:ss.SSS')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4dfa487a-36be-4a5b-999e-d393807b8c68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unixtimes = [(1393561800, ),\n",
    "             (1456713488, ),\n",
    "             (1514701799, ),\n",
    "             (1567189800, )\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "270f83e8-0bcc-4b8c-ae9a-2ee0445e3862",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unixtimesDF = spark.createDataFrame(unixtimes).toDF('unixtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "410b8395-c2d7-4848-afca-49ccbb22050a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+\n",
       "|  unixtime|\n",
       "+----------+\n",
       "|1393561800|\n",
       "|1456713488|\n",
       "|1514701799|\n",
       "|1567189800|\n",
       "+----------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+\n|  unixtime|\n+----------+\n|1393561800|\n|1456713488|\n|1514701799|\n|1567189800|\n+----------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "unixtimesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "44840073-4dd6-4ff3-9d85-bd56afd69f3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c88860b1-2d40-4325-b2c6-090df29fd1f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+--------+-------------------+\n",
       "|  unixtime|    date|               time|\n",
       "+----------+--------+-------------------+\n",
       "|1393561800|20140228|2014-02-28 04:30:00|\n",
       "|1456713488|20160229|2016-02-29 02:38:08|\n",
       "|1514701799|20171231|2017-12-31 06:29:59|\n",
       "|1567189800|20190830|2019-08-30 18:30:00|\n",
       "+----------+--------+-------------------+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+--------+-------------------+\n|  unixtime|    date|               time|\n+----------+--------+-------------------+\n|1393561800|20140228|2014-02-28 04:30:00|\n|1456713488|20160229|2016-02-29 02:38:08|\n|1514701799|20171231|2017-12-31 06:29:59|\n|1567189800|20190830|2019-08-30 18:30:00|\n+----------+--------+-------------------+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "unixtimesDF.\\\n",
    "  withColumn('date',from_unixtime('unixtime','yyyyMMdd')).\\\n",
    "  withColumn('time',from_unixtime('unixtime')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0243c9ae-ddc5-444d-82ef-8ac9fc20b79c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Date and Time Manipulation",
   "notebookOrigID": 1036212495696169,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
